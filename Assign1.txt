1. Define variables before you use them [5 points] 
2. Primal and dual forms of hard and soft margin SVMs [20 points] 
3. Concept of support vectors (two types) [10 points] 
4. Discussions on why max margin is good [5 points]
5. Experiments
(a) compare your w, b obtained via solving the primal problem, with the w, b reconstructed by the dual variables obtained via solving the dual problem (both the results and the reconstruction formulation) [5 points]

The solver and syntax to use for modeling a linear classify in a binary problem include:
linprog, theory, examples

whereas quadprog, theory, examples, fmincon, theory and examples are 

(b) check duality gap of yours (both the result and the formulation) [5 points]
(c) compare your w, b, α with those of libsvm [5 points]
(d) compare training and testing errors of your code and libsvm [5 points]
(e) code [40 points]

For very large data sets a feasible approach is to randomly choose a subset of the data set, conduct grid-search on them, and then do a better-region-only grid-search on the complete data set.
*guide.pdf p.8*

All SVM formulations supported in LIBSVM are quadratic minimization problems. 
LIBSVR
*libsvm.pdf p.2*

Two implementation techniques to reduce the running time for minimizing SVM quadratic problems: shrinking and caching. 
*libsvm.pdf p.2*

ν-support vector regression (ν-SVR)

All LIBSVM’s training and testing algorithms are implemented in the ﬁle svm.cpp. The two main sub-routines are svm train and svm predict. The training procedure is more sophisticated, so we give the code organization in Figure 1. From Figure 1, for classiﬁcation, svm train decouples a multi-class problem to two-class problems (see Section 7) and calls svm train one several times. For regression and one-class SVM, it directly calls svm train one. The probability outputs for classiﬁcation and regression are also handled in svm train. Then, according to the SVM formulation, svm train one calls a corresponding sub-routine such as solve c svc for C-SVC and solve nu svc for ν-SVC. All solve * sub-routines call the solver Solve after preparing suitable input values. The sub-routine Solve minimizes a general form of SVM optimization problems; see (11) and (22). Details of the sub-routine Solve are described in Sections 4-6.
*libsvm.pdf p.4*

Comparison of standard forms, dual problems, and optimal solutions for SVM formulations

Summary of SVM formulations in LIBSVM

Ref1_____________________________|Ref2_________________________________|Ref3_____________________________________________|Ref4_____________________________________________|Ref5_______________________________________________________|
C-Support Vector Classiﬁcation___|ν-Support Vector Classiﬁcation_______|Distribution Estimation (One-class SVM)__________|\epsilon-Support Vector Regression (\epsilon-SVR)|ν-Support Vector Regression (ν-SVR)________________________|
*libsvm.pdf p.3-7*

Performance Measures: Accuracy, MSE (mean squared error) and r2 (squared correlation coeﬃcient).

All LIBSVM’s training and testing algorithms are implemented in the ﬁle svm.cpp. 
The two main sub-routines are svm train and svm predict.
Then, according to the SVM formulation, svm train one calls a corresponding sub-routine such as solve c svc for C-SVC and solve nu svc for ν-SVC.

The sub-routine Solve minimizes a general form of SVM optimization problems; see (11) and (22). Details of the sub-routine Solve are described in Sections 4-6.

optimization for one linear constraint:  C-SVC, -SVR, and One-class SVM
*libsvm.pdf p.10-15*

optimization for two linear constraint:  νSVC and ν-SVR
*libsvm.pdf p.15*

A.1 Astroparticle Physics • Original sets with default parameters
$ ./svm-train svmguide1 $ ./svm-predict svmguide1.t svmguide1.model svmguide1.t.predict → Accuracy = 66.925% • Scaled sets with default parameters
$ ./svm-scale -l -1 -u 1 -s range1 svmguide1 > svmguide1.scale $ ./svm-scale -r range1 svmguide1.t > svmguide1.t.scale $ ./svm-train svmguide1.scale $ ./svm-predict svmguide1.t.scale svmguide1.scale.model svmguide1.t.predict → Accuracy = 96.15% • Scaled sets with parameter selection (change to the directory tools, which contains grid.py)
$ python grid.py svmguide1.scale ··· 2.0 2.0 96.8922
(Best C=2.0, γ=2.0 with ﬁve-fold cross-validation rate=96.8922%)
$ ./svm-train -c 2 -g 2 svmguide1.scale $ ./svm-predict svmguide1.t.scale svmguide1.scale.model svmguide1.t.predict → Accuracy = 96.875% • Using an automatic script
$ python easy.py svmguide1 svmguide1.t Scaling training data... Cross validation...
9
Best c=2.0, g=2.0 Training... Scaling testing data... Testing... Accuracy = 96.875% (3875/4000) (classification)


Using the same scaling factors for training and testing sets, we obtain much better accuracy.
$ ../svm-scale -l 0 -s range4 svmguide4 > svmguide4.scale $ ../svm-scale -r range4 svmguide4.t > svmguide4.t.scale $ python easy.py svmguide4.scale svmguide4.t.scale Accuracy = 89.4231% (279/312) (classification)

• RBF kernel with parameter selection
$ cat leu leu.t > leu.combined $ python grid.py leu.combined ··· 8.0 3.0517578125e-05 97.2222
(Best C=8.0, γ = 0.000030518 with ﬁve-fold cross-validation rate=97.2222%)
• Linear kernel with parameter selection
$ python grid.py -log2c -1,2,1 -log2g 1,1,1 -t 0 leu.combined ··· 0.5 2.0 98.6111
(Best C=0.5 with ﬁve-fold cross-validation rate=98.61111%)
Though grid.py was designed for the RBF kernel, the above way checks various C using the linear kernel (-log2g 1,1,1 sets a dummy γ).

LIBLINEAR is eﬃcient for large-scale document classiﬁcation. Let us consider a large set rcv1 test.binary with 677,399 instances.
$ time liblinear-1.21/train -c 0.25 -v 5 rcv1_test.binary Cross Validation Accuracy = 97.8538% 68.84s

 Consider the data http://www.csie. ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/covtype.libsvm.binary.scale. bz2. The number of instances 581,012 is much larger than the number of features 54. We run LIBLINEAR with -s 1 (default) and -s 2.
$ time liblinear-1.21/train -c 4 -v 5 -s 2 covtype.libsvm.binary.scale Cross Validation Accuracy = 75.67% 67.224s $ time liblinear-1.21/train -c 4 -v 5 -s 1 covtype.libsvm.binary.scale
14
Cross Validation Accuracy = 75.6711% 452.736s
Clearly, using -s 2 leads to shorter training time.

We have shown that the KKT condition of problem (22) implies Eqs. (25) and (26) according to yi = 1 and −1, respectively.
Now we consider the case of yi = 1. If there exists αi such that 0 < αi < C, then we obtain r1 = ∇if(α). In LIBSVM, for numerical stability, we average these values. 

*guide.pdf p.8-14*

We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance ), the code without using shrinking may be much faster. In this situation, because of the small number of iterations, the time spent on all decomposition iterations can be even less than one single gradient reconstruction. 
*libsvm.pdf p.25*

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-train.exe a2a.train
*
optimization finished, #iter = 796
nu = 0.461826
obj = -971.667542, rho = 0.585499
nSV = 1068, nBSV = 1028
Total nSV = 1068

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>dir
 驱动器 C 中的卷是 OS
 卷的序列号是 F27A-BA5A

 C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows 的目录

2019/08/22  11:19    <DIR>          .
2019/08/22  11:19    <DIR>          ..
2019/08/22  10:45         2,167,822 a2a.test
2019/08/22  10:43           162,053 a2a.train
2019/08/22  11:19            77,993 a2a.train.model
2019/08/22  11:04           258,048 libsvm.dll
2019/08/22  11:04            14,336 libsvmread.mexw64
2019/08/22  11:03            13,312 libsvmwrite.mexw64
2019/08/22  11:04           211,968 svm-predict.exe
2019/08/22  11:04           165,376 svm-scale.exe
2019/08/22  11:03           226,816 svm-toy.exe
2019/08/22  11:04           247,808 svm-train.exe
2019/08/22  11:04            28,160 svmpredict.mexw64
2019/08/22  11:04            68,608 svmtrain.mexw64
              12 个文件      3,642,300 字节
               2 个目录 423,082,975,232 可用字节

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-predict.exe
Usage: svm-predict [options] test_file model_file output_file
options:
-b probability_estimates: whether to predict probability estimates, 0 or 1 (default 0); for one-class SVM only 0 is supported
-q : quiet mode (no outputs)

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-predict.exe a2a.test a2a.train.model a2a.out
Accuracy = 83.9781% (25442/30296) (classification)

 We can divide this process broadly into 4 stages. Each stage requires a certain amount of time to execute:

Loading and pre-processing Data – 30% time
Defining Model architecture – 10% time
Training the model – 50% time
Estimation of performance – 10% time

Soft Margin Binary SVM
- The primal form and dual form for both soft and hard margin SVM
- Concept of support vectors
- Why max margin is good
- Concepts of generalisation/test error
- Experimental results including comparison between your implementation and libsvm





C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-train.exe a2a.train
*
optimization finished, #iter = 796
nu = 0.461826
obj = -971.667542, rho = 0.585499
nSV = 1068, nBSV = 1028
Total nSV = 1068

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>dir
 驱动器 C 中的卷是 OS
 卷的序列号是 F27A-BA5A

 C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows 的目录

2019/08/22  11:19    <DIR>          .
2019/08/22  11:19    <DIR>          ..
2019/08/22  10:45         2,167,822 a2a.test
2019/08/22  10:43           162,053 a2a.train
2019/08/22  11:19            77,993 a2a.train.model
2019/08/22  11:04           258,048 libsvm.dll
2019/08/22  11:04            14,336 libsvmread.mexw64
2019/08/22  11:03            13,312 libsvmwrite.mexw64
2019/08/22  11:04           211,968 svm-predict.exe
2019/08/22  11:04           165,376 svm-scale.exe
2019/08/22  11:03           226,816 svm-toy.exe
2019/08/22  11:04           247,808 svm-train.exe
2019/08/22  11:04            28,160 svmpredict.mexw64
2019/08/22  11:04            68,608 svmtrain.mexw64
              12 个文件      3,642,300 字节
               2 个目录 423,082,975,232 可用字节

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-predict.exe
Usage: svm-predict [options] test_file model_file output_file
options:
-b probability_estimates: whether to predict probability estimates, 0 or 1 (default 0); for one-class SVM only 0 is supported
-q : quiet mode (no outputs)

C:\Users\wendy\Desktop\libsvm3.2.3\libsvm-3.23\windows>svm-predict.exe a2a.test a2a.train.model a2a.out
Accuracy = 83.9781% (25442/30296) (classification)

The SVM should maximize the distance between the two decision boundaries. Mathematically, this means we want to maximize the distance between the hyperplane defined by wTx+b=−1 and the hyperplane defined by wTx+b=1. This distance is equal to 2∥w∥. This means we want to solve maxw2∥w∥. Equivalently we want minw∥w∥2.

The SVM should also correctly classify all x(i), which means y(i)(wTx(i)+b)≥1,∀i∈{1,…,N}

Which leads us to the following quadratic optimization problem:

minw,bs.t.∥w∥2,y(i)(wTx(i)+b)≥1∀i∈{1,…,N}

This is the hard-margin SVM, as this quadratic optimization problem admits a solution iff the data is linearly separable.

One can relax the constraints by introducing so-called slack variables ξ(i). Note that each sample of the training set has its own slack variable. This gives us the following quadratic optimization problem:

minw,bs.t.∥w∥2+C∑i=1Nξ(i),y(i)(wTx(i)+b)≥1−ξ(i),ξ(i)≥0,∀i∈{1,…,N}∀i∈{1,…,N}

This is the soft-margin SVM. C is a hyperparameter called penalty of the error term. (What is the influence of C in SVMs with linear kernel? and Which search range for determining SVM optimal parameters?).

One can add even more flexibility by introducing a function ϕ that maps the original feature space to a higher dimensional feature space. This allows non-linear decision boundaries. The quadratic optimization problem becomes:

minw,bs.t.∥w∥2+C∑i=1Nξ(i),y(i)(wTϕ(x(i))+b)≥1−ξ(i),ξ(i)≥0,∀i∈{1,…,N}∀i∈{1,…,N}

Optimization
The quadratic optimization problem can be transformed into another optimization problem named the Lagrangian dual problem (the previous problem is called the primal):

maxαs.t.minw,b∥w∥2+C∑i=1Nα(i)(1−wTϕ(x(i))+b)),0≤α(i)≤C,∀i∈{1,…,N}

This optimization problem can be simplified (by setting some gradients to 0) to:

maxαs.t.∑i=1Nα(i)−∑i=1N∑j=1N(y(i)α(i)ϕ(x(i))Tϕ(x(j))y(j)α(j)),0≤α(i)≤C,∀i∈{1,…,N}

w doesn't appear as w=∑Ni=1α(i)y(i)ϕ(x(i)) (as stated by the representer theorem).

We therefore learn the α(i) using the (x(i),y(i)) of the training set.

(FYI: Why bother with the dual problem when fitting SVM? short answer: faster computation + allows to use the kernel trick, though there exist some good methods to train SVM in the primal e.g. see {1})

Making a prediction
Once the α(i) are learned, one can predict the class of a new sample with the feature vector xtest as follows:

ytest=sign(wTϕ(xtest)+b)=sign(∑i=1Nα(i)y(i)ϕ(x(i))Tϕ(xtest)+b)

The summation ∑Ni=1 could seem overwhelming, since it means one has to sum over all the training samples, but the vast majority of α(i) are 0 (see Why are the Lagrange multipliers sparse for SVMs?) so in practice it isn't an issue. (note that one can construct special cases where all α(i)>0.) α(i)=0 iff x(i) is a support vector. The illustration above has 3 support vectors.

Kernel trick
One can observe that the optimization problem uses the ϕ(x(i)) only in the inner product ϕ(x(i))Tϕ(x(j)). The function that maps (x(i),x(j)) to the inner product ϕ(x(i))Tϕ(x(j)) is called a kernel, a.k.a. kernel function, often denoted by k.

One can choose k so that the inner product is efficient to compute. This allows to use a potentially high feature space at a low computational cost. That is called the kernel trick. For a kernel function to be valid, i.e. usable with the kernel trick, it should satisfy two key properties. There exist many kernel functions to choose from. As a side note, the kernel trick may be applied to other machine learning models, in which case they are referred as kernelized.


Suppose we have training vectors Xi in real number R^l , 
with the constraints that numbers can be separable with a linear classifier for a binary classification problem(y are the labels, x are the features in R, i=1, 2, 3,...l). 
We write our classifier as g(x)=y_i(W^T*Xi + b) and this produces either 1 or -1 to represent which side of the hyperplane does X falls into.
The value of objective function f(x)= W^T*Xi + b could be :
W^T*Xi + b>=0, then g(x)=1 (1)
W^T*Xi + b<0, then g(x)=-1 (2)
The functional margin is then defined as to find the optimised slope and intercept of (w,b) so that the prediction errors are minimal.
Optimised separating hyperplane (OSH) can be obtained from the middle point of two parallel support hyperplanes, 
which generate maximum margin in dividing the two classes of features. As demonstrated in the figure below:
We can also visualise the support hyperplanes as pushing two wood boards from OSH to each side until one nearest feature lands on the hyperplane, and making sure 
as many features are correctly labeled.
The distance between separating hyperplane and two support hyperplane can be written as:
d = (||b+1|-|b||)/||w||=1/||w||, if b<=-1, or b>=0; (4)
d = (||b+1|+|b||)/||w||=1/||w||, if -1<b<0. (5)
margin = 2d = 2/||w||, therefore minimising w can create maximum margin. 
Each side of equation (1) and (2) still exist when multiplying a constant, 
therefore infinite groups of anwers may satisfy the conditions of (1) and (2).
To simplify the hypothesis, we times f(x) with g(x) and thus combine (1) and (2) to the following equation:
y_i(W^T*Xi + b)-1 >= 0, for yi=1, i=1,2,3,...,m       (3)
To solve the primal optimization problem, we consider introducing Lagrange multipliers alpha for optimising w and b together as a joint set. 
If we define (3) as g_i(\theta)<=0, the primal optimum problem is turned into :
min_{w,b}||w||^2/2 =f({\theta}) (6)
s.t. y_i(W^T*Xi + b)-1 >= C{\sum_{i=1}^{n}}{\kesi}, i=1,2,3,...,m (7)
{\kesi} \geq or 
\ge 0, i=1,2,...,l (8)

We can convert the above equation using Lagrange Multiplier Method, to find the w*, alpha*, beta* that can minimise the L values:

when {y}_i({w}^T{x}_i+b)-1=0, {a}_i>=0;(vectors on the support hyperplanes)
when {y}_i({w}^T{x}_i+b)-1>0, {a}_i=0; (points away from the OSH zone)

To find minimum L value, we have summarized the Lagrange multiplier condition as:
w*=Sum{1->N}{a}_i{y}_i{x}_i
Sum{1->N}{a}_i{y}_i=0
{y}_i({w}^T{x}_i+b)>=1
Lagrange multiplier condition:
{a}_i>=0
Complementary slackness
{a}_i[{y}_i({w}^T{x}_i+b)-1]=0

The points that sit on the support hyperplanes are defined as support vectors.
when {y}_i({w}^T{x}_i+b)-1=0, {a}_i=0;

After confirming the support vectors, we can further infer the classification of the new points with the following:

f(x)=({w}^T{x}_i+b)=Sum{1->i}{a}_i{y}_i{x}_i^Tx+b 
b= {y}_i-Sum{1->j}{a}_j{y}_j{x}_j^T{x}_i 

比较一下LIBSVM在python和commandline以及matlab之间的差异并说明为什么
Compare the running results between python, windows commandline and matlab and explain why

Dataset in use should be MNIST or easy to use ML library

The goal of this assignment is to learn how to train SVM classifier on image data wth use of SVM from 




We hope to minimize the Lagrangian factor \kesi towards 0 so that the precondition of g_i(theta) <=0 can be satisfied.
When g_i(theta) >0, the objective functional becomes non-convex and the optimisation problem cannot be resolved. 
Please note the optimization problem has a convex quadratic objective and only linear constraints expressed as algebraic equations. 
In real scenarios it is challenging to derive an efficient algorithm and solve the problem in very high dimensional spaces. 

We can transform the above equations (6), (7), (8) through applying Lagrange multipliers method in (9).  To find partial derivative of (6) + (7)
so that \frac {\vartheta f}{\vartheta x}= \frac {\vartheta f}{\vartheta y}=0. The minimum value of (6) + (7) can then be calculated when (9)=0.
h_i({\theta})=L({\theta}, b, {\kesi} )=(6) +(7)=0  (9)

 
 
explained as to build separating hyperplane and minimise its errors with regularisation factor:

min(w, b, )

Draft___________________________________________________________________________________________________________________________________________AB


Assignment 1

Support Vector Machine 

This assignment attempts to explain basic principles for implementation of both hard and soft margin SVM algorithm in Python using the CVXOPT library, and comparing those with 
the results from libsvms. We derive notations and inputs for the API from vectorised matrix, through formulations of the primal and dual optimization problems.

Background

SVM algorithms construct a set of hyperplanes for classification or regression tasks. A number of hyperplanes may satisfy the separation task therefore the best hyperplane 
represents the largest margin between different classes of datasets. SVMs focus only on the points that are most hard to tell apart, whereas other classifiers may compute all the points. 
Firstly SVMs calculate the distances between vectors and try to find two vectors that are nearest to each other. Through vector subtraction A-B, 
a perpendicular plane is constructed to bisect AB as the optimal separating hyperplane. 

In a typical linear two-class problem, separation task can be solved by building linear classifier g({w^T}x_{i}+b)= H_{i}{g} with labels y and features x.  g(z)=1 if z>=0, and g(z)=-1 otherwise. 
y belongs to {-1,1} and "w, b" notation represents the slope and intercept terms. i = 1,..., m. 



Suppose we have training vectors Xi in real number R^l , 
with the constraints that numbers can be separable with a linear classifier for a binary classification problem(y are the labels, x are the features in R, i=1, 2, 3,...l). 
We write our classifier as g(x)=y_i(W^T*Xi + b) and this produces either 1 or -1 to represent which side of the hyperplane does X falls into.
The value of objective function f(x)= W^T*Xi + b could be :
W^T*Xi + b>=0, then g(x)=1 (1)
W^T*Xi + b<0, then g(x)=-1 (2)
The functional margin is then defined as to find the optimised slope and intercept of (w,b) so that the prediction errors are minimal.
Optimised separating hyperplane (OSH) can be obtained from the middle point of two parallel support hyperplanes, 
which generate maximum margin in dividing the two classes of features. As demonstrated in the figure below:


The distance between separating hyperplane and two support hyperplane can be written as:
d = (||b+1|-|b||)/||w||=1/||w||, if b<=-1, or b>=0; (4)
d = (||b+1|+|b||)/||w||=1/||w||, if -1<b<0. (5)
margin = 2d = 2/||w||, therefore minimising w can create maximum margin. 
Each side of equation (1) and (2) still exist when multiplying a constant, 
therefore infinite groups of anwers may satisfy the conditions of (1) and (2).
To simplify the hypothesis, we times f(x) with g(x) and thus combine (1) and (2) to the following equation:
y_i(W^T*Xi + b)-1 >= 0, for yi=1, i=1,2,3,...,m       (3)

The primal optimisation problem is to find the optimal margin classifer that:
 min_{w,b} 1/2||w||_2, 
 s.t. g_{i}(w)=-y^{i}({w^T}x_{i}+b)+1<=0, i = 1,..., m. 


To solve the primal optimization problem, we consider introducing Lagrange multipliers alpha for optimising w and b together as a joint set. 
If we define (3) as g_i(\theta)<=0, the primal dual optimum problem is turned into :
min_{w,b}||w||^2/2 =f({\theta}) (6)
s.t. y_i(W^T*Xi + b)-1 >= C{\sum_{i=1}^{n}}{\kesi}, i=1,2,3,...,m (7)
{\kesi} \geq or \ge 0, i=1,2,...,l (8)

We hope to minimize the Lagrangian factor \kesi towards 0 so that the precondition of g_i(theta) <=0 can be satisfied.
When g_i(theta) >0, the objective functional becomes non-convex and the optimisation problem cannot be resolved. 
Please note the optimization problem has a convex quadratic objective and only linear constraints expressed as algebraic equations. 
In real scenarios it is challenging to derive an efficient algorithm and solve the problem in very high dimensional spaces. 

We can transform the above equations (6), (7), (8) through applying Lagrange multipliers method in (9).  To find partial derivative of (6) + (7)
so that \frac {\vartheta f}{\vartheta x}= \frac {\vartheta f}{\vartheta y}=0. The minimum value of (6) + (7) can then be calculated when (9)=0.
h_i({\theta})=L({\theta}, b, {\kesi} )=(6) +(7)=0  (9)

alpha*_{i}g_{i}(w*) represents the KTT dual complementarity condition if some w*, alpha*, beta* satisfy the condition, which shows that SVMs lead to convergence test and only a small number of 
support vectors can be found.

Libsvms present the calculation into 
create P where H(i,j)=y^{i}y^{g}(x^{i}x^{g}).    can be linearly separable, binary classification scenarios,  
SVM algorithm 



